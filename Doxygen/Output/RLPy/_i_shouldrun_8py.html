<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.3.1"/>
<title>RLPy: Getting Started: A 10-15 Minute Introduction</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ACL_logo.png"/></td>
  <td style="padding-left: 0.5em;">
   <div id="projectname">RLPy
   &#160;<span id="projectnumber">Version 1.0</span>
   </div>
   <div id="projectbrief">The Reinforcement Learning Library for Education and Research</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.3.1 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="install_8txt.html#the_install_page"><span>Installation</span></a></li>
      <li><a href="_i_shouldrun_8py.html"><span>Tutorial</span></a></li>
      <li><a href="_f_a_q_8txt.html"><span>Frequently&#160;Asked&#160;Questions&#160;(FAQ)</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="_code.html"><span>Code</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_i_shouldrun_8py.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Classes</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Namespaces</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Variables</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&#160;</span>Pages</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Getting Started: A 10-15 Minute Introduction </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="First_Run"></a>
First Run</h1>
<p>Welcome to RLPy!<br/>
 If you receive errors during any of the steps below, please refer to install.txt for solutions to common issues. <br/>
 <br/>
</p>
<p>Begin by opening IShouldRun.py. You will notice a series of parameters at the top of the file, followed by assignments to each of the functional components shown in <a class="el" href="index.html#big_pic">The Big Picture</a>: domain, representation, policy, agent, and experiment.<br/>
 Leave these alone for now; just run the file as-is. You should see something like the following:</p>
<div class="image">
<img src="gridWorld_learning.png" alt="gridWorld_learning.png"/>
<div class="caption">
GridWorld Domain Visualization (IShouldRun.py)</div></div>
<p> This is a visual representation of the domain (here, "GridWorld") and is useful in quickly judging or demonstrating the performance of an experiment. The objective on this domain is to move the agent (triangle) from the start (blue) to the goal (green) location in the shortest distance possible, while avoiding the pits (red); -0.001 reward is applied for every step, and reaching the goal or pit regions give rewards of +1.0 and -1.0 respectively, terminating the episode. <br/>
 <br/>
 On the left, you can see the learned policy in action after each 200 steps of training data. Notice how the policy gradually converges to the optimal, direct route which avoids pits. <br/>
 On the right, you can see the representation of the value function overlayed on the domain. Notice how after successive iterations, the agent learns the high (green) value of being in states that lie along the optimal path, even though they offer no immediate reward. It also learns the low (red) value of unimportant / undesirable states.<br/>
 The set of possible actions in each grid is highlighted by arrows, where the size of arrows correspond to the Q(s,a). The best action is shown as black. If the agent hasn't learned the optimal policy in some grid cells (e.g. Row 2, Column 1), it has not explored enough to learn the correct action ('left' in Row 2, Column 1). It likely still performs well though, since such states are only ever reached because of random noise in the agent's actions.<br/>
 <br/>
 Each domain in RLPy offers visualization like that shown on the left, and where possible, a representation of the value function like that shown on the right as well.</p>
<h1><a class="anchor" id="Interpreting_Output"></a>
Interpreting Output</h1>
<p>In the console window you should see output similar to the following:<br/>
 ...<br/>
 647: E[0:00:01]-R[0:00:15]: Return=+0.97, Steps=33, Features = 20<br/>
 1000 &gt;&gt;&gt; E[0:00:04]-R[0:00:37]: Return=+0.99, Steps=11, Features = 20<br/>
 1810: E[0:00:05]-R[0:00:23]: Return=+0.98, Steps=19, Features = 20<br/>
 ...<br/>
</p>
<p>The meaning of each component is shown in the figure. Note that a "performance run" (indicated by "&gt;&gt;&gt;" in the output window) tests the agent using its latest policy, without any exploration or modifications that might be used during learning (such as the randomization of the episilon-greedy policy). The visualization shows these performance runs. <br/>
 (Note: If you see <em>only</em> performance runs as console output, this simply means that your machine is completing the learning before the performance run faster than the console logging rate, usually 1 Hz, and does not indicate a problem.)</p>
<div class="image">
<img src="rlpy_output.png" alt="rlpy_output.png"/>
<div class="caption">
Sample Output (IShouldRun.py)</div></div>
<p> After the cutoff of 2,000 steps specified in IShouldRun.py, the experiment is complete, and a second figure window appears, showing the reward earned on each performance run. On this domain, an excellent policy is located almost immediately (reward 0.989).<br/>
 The final plot likely shows enormous variance in reward obtained on performance runs - this is because the GridWorld domain has a default noise level of 0.3, meaning that 30% of the time, a random action will be taken; at the start, when only two actions are available, this corresponds to a 30% failure rate (-1 reward), even when the optimal policy has been found.<br/>
 You can adjust the noise parameter "NOISE" at the top of IShouldRun.py; just be aware that the agent does not explore (epsilon=0) during performance runs, so with 0 noise and a bad initial policy, you may have to sit and watch the agent execute a (likely oscillatory) deterministic policy for all 1000 steps of an episode!<br/>
</p>
<h1><a class="anchor" id="Analyzing_Data"></a>
Analyzing Data</h1>
<p>The variable PROJECT_PATH in IShouldRun.py determines the directory in which all results are stored; IShouldRun.py defaults to ./Results/IShouldRun. A folder for a particular experiment is automatically generated in that directory based on its parameters; here, "GridWorld-Tabular-2000" for the GridWorld domain, Tabular representation, with 2,000 steps total. <br/>
 The data logged to the console is stored in a file "#-out.txt", where "#" is the ID of the experiment, here "1". Open this "1-out.txt" and verify this. <br/>
 Data is stored in a more compact form in "#-results.txt", described below. <br/>
 <br/>
 Now open the file "IShouldMerge.py" and run it. (If you get an error which includes "No directory including result was found at ['Results/IShouldRun']", make sure you allow IShouldRun.py to run to completion, when the second figure window appears, approximately 30 seconds). <br/>
 You should now see several figures which demonstrate various performance measures of the experiment, such as return vs. number of steps. The "paths" variable in this file specifies a directory to recursively search to generate these figures (and where to store them); if multiple results directories are found, they are plotted simultaneously against each other on the same figure. If multiple results files are found in the same directory (e.g. 1-results.txt, 2-results.txt, etc., generated using multipleRuns.py) the mean curve is drawn with variance around it. <br/>
 <br/>
 </p>
<h1><a class="anchor" id="Inv_Pend"></a>
A Slightly More Challenging Domain: Inverted Pendulum</h1>
<p>The small GridWorld domain is easy to understand, but does not produce meaningful final plots because the optimal policy is learned so quickly. We will now run an experiment on the Inverted Pendulum Domain, where the goal is to prevent the pendulum from falling below the horizontal by applying clockwise (red) or counterclockwise (black) torque. No reward is received for balancing and there is no control penalty; a penalty is only applied if the pendulum falls, after which the episode terminates.<br/>
 <br/>
 Return to IShouldRun.py, and:<br/>
 1) Near the top (line 33), change the following two variables to allow more steps for learning the domain: <br/>
 PERFORMANCE_CHECKS = 5 <br/>
 LEARNING_STEPS = 8000 <br/>
 <br/>
 2) Scroll to the "Domain" section (line 77), comment the line corresponding to GridWorld and decomment the line corresponding to Pendulum_InvertedBalance. <br/>
 Now, run IShouldRun.py and observe the new domain. Initially, the agent fails to balance the pendulum, but after approximately 6000 steps, can do so reliably up to the maximum number of steps allowed for an episode, 300. <br/>
</p>
<div class="image">
<img src="pendulum_learning.png" alt="pendulum_learning.png"/>
<div class="caption">
Pendulum Visualization, Value Function, and Policy after 8000 steps (IShouldRun.py)</div></div>
<p> The value function (center), which plots pendulum angular rate against its angle, demonstrates the highly undesirable states of a steeply inclined pendulum (near the horizontal) with high angular velocity in the direction in which it is falling. <br/>
 The policy (right) initially appears random, but converges to the shape shown, with distinct black (counterclockwise torque action) and red (clockwise action) regions in the first and third quadrants respectively, and a white stripe along the major diagonal between. This makes intuitive sense; if the pendulum is left of center and/or moving counterclockwise (third quadrant), for example, a corrective clockwise torque action should certainly be applied. The white stripe in between shows that no torque should be applied to a balanced pendulum with no angular velocity, or if it lies off-center but has angular velocity towards the balance point. <br/>
</p>
<p><br/>
 To allow the experiment to run more quickly, without visualization: <br/>
 1) Assign SHOW_PERFORMANCE = 0 (surpresses visualization) <br/>
 2) Assign PERFORMANCE_CHECKS = 10 <br/>
 3) Assign LEARNING_STEPS = 20000 <br/>
 4) Assign domain = Pendulum_InvertedBalance(episodeCap = 3000, logger = logger); <br/>
 (Step 4 extends the maximum number of steps per episode from 300 to 3000 to make the task slightly more challenging.) <br/>
 Now, re-run the experiment.<br/>
 Finally, return to IShouldMerge.py, comment the old "paths" variable, and decomment the one corresponding to the Pendulum results. The learning up to approximately 2,000 steps should be apparent through continuously improving performance. Large fluctuations in reward due to chance are smoothed by running the experiment many times using multipleRuns.py, described in another tutorial. <br/>
</p>
<h1><a class="anchor" id="Conclusion"></a>
Conclusion</h1>
<p>We have seen how to run experiments, interpret visualization, and generate results on two classic Reinforcement Learning domains. We have also seen the structure of the high-level files;<br/>
 IShouldRun.py &lt;&ndash;&gt; main.py - Identical, former has excess options removed. You may now try using main.py instead of IShouldRun.py, experimenting with different agents, representations, etc., all of which have parameters defined at the top of main.py. <br/>
 IShouldMerge.py &lt;&ndash;&gt; mergeRuns.py - Identical, former just has "paths" prespecified. You should now use mergeRuns.py to generate your results. <br/>
 Finally, you should explore the rest of the structure of the framework, and perhaps try implementing domains and algorithms of your own. <br/>
 <br/>
</p>
<p>If you have questions, email:<br/>
 rlpy[at]mit[dot]edu<br/>
 <br/>
 <br/>
 <em> The only real mistake is the one from which we learn nothing. </em> -John Powell </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-40370665-1']);
  _gaq.push(['_trackPageview']);
  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.3.1 </li>
  </ul>
</div>
</body>
</html>
