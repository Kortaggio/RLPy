============================================================
Domain:		Pendulum_InvertedBalance
Dimensions:	2
|S|:		inf
|A|:		3
|S|x|A|:		inf
Episode Cap:	3000
Gamma:		0.95
============================================================
Representation:		IndependentDiscretizationCompactBinary
Features per action:	41
Discretization:		20
Starting Features:	41
Aggregated States:	400
Binary Dimensions:	[]
============================================================
Representation:		iFDD
Features per action:	41
Discretization:		20
Starting Features:	41
Aggregated States:	400
Initial Representation:	IndependentDiscretizationCompactBinary
Plus:			1
Sparsify:		1
Cached:			1
Online Threshold:	0.020
Batch Threshold:		0.020
Max Batch Discovery:	1
============================================================
Agent:		LSPI
Policy:		eGreedy
Max LSPI Iterations:	5
Max Data Size:		10000
Steps Between LSPI run:	1000
Weight Difference tol.:	0.001
Track the best policy:	0
Use Sparse:		0
Max Representation Expansion Iterations:	50
============================================================
Experiment:		OnlineExperiment
Output:			././5-results.txt
Learning Steps:		10000
Performance Checks:	10
Log Intervals:		60(s)
============================
Running LSPI with 1000 Samples
============================
-----------------
Representation Expansion iteration #1
-----------------
Total LSTD Time = 0(s)
Running Policy Iteration:
1: 0(s), ||w1-w2|| = 0.0126, Sparsity=98.4%, -1.000 Return, 10 Steps, 41 Features
2: 0(s), ||w1-w2|| = 0.0000, Sparsity=98.4%, -1.000 Return, 7 Steps, 41 Features
Total Policy Iteration Time = 0(s)
iFDD Batch: Max Relevance = 0.014
============================
1000 >>> E[0:00:01]-R[0:00:09]: Return=-1.00, Steps=10, Features = 41
============================
Running LSPI with 2000 Samples
============================
-----------------
Representation Expansion iteration #1
-----------------
Total LSTD Time = 1(s)
Running Policy Iteration:
1: 0(s), ||w1-w2|| = 0.0055, Sparsity=98.3%, -1.000 Return, 8 Steps, 41 Features
2: 0(s), ||w1-w2|| = 0.0000, Sparsity=98.3%, -1.000 Return, 12 Steps, 41 Features
Total Policy Iteration Time = 0(s)
iFDD Batch: Max Relevance = 0.014
============================
2000 >>> E[0:00:03]-R[0:00:10]: Return=-1.00, Steps=10, Features = 41
============================
Running LSPI with 3000 Samples
============================
-----------------
Representation Expansion iteration #1
-----------------
Total LSTD Time = 1(s)
Running Policy Iteration:
1: 0(s), ||w1-w2|| = 0.0030, Sparsity=98.3%, -1.000 Return, 7 Steps, 41 Features
2: 0(s), ||w1-w2|| = 0.0000, Sparsity=98.3%, -1.000 Return, 14 Steps, 41 Features
Total Policy Iteration Time = 0(s)
iFDD Batch: Max Relevance = 0.008
============================
3000 >>> E[0:00:05]-R[0:00:11]: Return=-1.00, Steps=10, Features = 41
============================
Running LSPI with 4000 Samples
============================
-----------------
Representation Expansion iteration #1
-----------------
Total LSTD Time = 2(s)
Running Policy Iteration:
1: 0(s), ||w1-w2|| = 0.0023, Sparsity=98.3%, -1.000 Return, 12 Steps, 41 Features
2: 0(s), ||w1-w2|| = 0.0000, Sparsity=98.3%, -1.000 Return, 11 Steps, 41 Features
Total Policy Iteration Time = 0(s)
iFDD Batch: Max Relevance = 0.005
============================
4000 >>> E[0:00:07]-R[0:00:11]: Return=-1.00, Steps=12, Features = 41
============================
Running LSPI with 5000 Samples
============================
-----------------
Representation Expansion iteration #1
-----------------
Total LSTD Time = 2(s)
Running Policy Iteration:
1: 0(s), ||w1-w2|| = 0.0023, Sparsity=98.3%, -1.000 Return, 14 Steps, 41 Features
2: 0(s), ||w1-w2|| = 0.0001, Sparsity=98.3%, -1.000 Return, 16 Steps, 41 Features
Total Policy Iteration Time = 0(s)
iFDD Batch: Max Relevance = 0.005
============================
5000 >>> E[0:00:10]-R[0:00:10]: Return=-1.00, Steps=7, Features = 41
============================
Running LSPI with 6000 Samples
============================
-----------------
Representation Expansion iteration #1
-----------------
Total LSTD Time = 3(s)
Running Policy Iteration:
1: 0(s), ||w1-w2|| = 0.0101, Sparsity=98.2%, -1.000 Return, 11 Steps, 41 Features
2: 0(s), ||w1-w2|| = 0.0000, Sparsity=98.2%, -1.000 Return, 12 Steps, 41 Features
Total Policy Iteration Time = 0(s)
iFDD Batch: Max Relevance = 0.014
============================
6000 >>> E[0:00:14]-R[0:00:09]: Return=-1.00, Steps=8, Features = 41
============================
Running LSPI with 7000 Samples
============================
-----------------
Representation Expansion iteration #1
-----------------
Total LSTD Time = 3(s)
Running Policy Iteration:
1: 0(s), ||w1-w2|| = 0.0471, Sparsity=98.2%, -1.000 Return, 13 Steps, 41 Features
2: 0(s), ||w1-w2|| = 0.0000, Sparsity=98.2%, -1.000 Return, 10 Steps, 41 Features
Total Policy Iteration Time = 0(s)
iFDD Batch: Max Relevance = 0.047
New Feature 41: [5, 21], Relevance = 0.047
-----------------
Representation Expansion iteration #2
-----------------
Total LSTD Time = 3(s)
Running Policy Iteration:
1: 0(s), ||w1-w2|| = 0.0228, Sparsity=98.3%, -1.000 Return, 9 Steps, 42 Features
2: 0(s), ||w1-w2|| = 0.0000, Sparsity=98.3%, -1.000 Return, 12 Steps, 42 Features
Total Policy Iteration Time = 0(s)
iFDD Batch: Max Relevance = 0.007
============================
7000 >>> E[0:00:22]-R[0:00:09]: Return=-1.00, Steps=12, Features = 42
============================
Running LSPI with 8000 Samples
============================
-----------------
Representation Expansion iteration #1
-----------------
Total LSTD Time = 4(s)
Running Policy Iteration:
1: 0(s), ||w1-w2|| = 0.0232, Sparsity=98.2%, -1.000 Return, 12 Steps, 42 Features
2: 0(s), ||w1-w2|| = 0.0000, Sparsity=98.2%, -1.000 Return, 9 Steps, 42 Features
Total Policy Iteration Time = 0(s)
iFDD Batch: Max Relevance = 0.007
============================
8000 >>> E[0:00:26]-R[0:00:07]: Return=-1.00, Steps=10, Features = 42
============================
Running LSPI with 9000 Samples
============================
-----------------
Representation Expansion iteration #1
-----------------
Total LSTD Time = 4(s)
Running Policy Iteration:
1: 0(s), ||w1-w2|| = 0.0238, Sparsity=98.2%, -1.000 Return, 13 Steps, 42 Features
2: 0(s), ||w1-w2|| = 0.0003, Sparsity=98.2%, -1.000 Return, 10 Steps, 42 Features
Total Policy Iteration Time = 0(s)
iFDD Batch: Max Relevance = 0.008
============================
9000 >>> E[0:00:31]-R[0:00:03]: Return=-1.00, Steps=11, Features = 42
============================
Running LSPI with 10000 Samples
============================
-----------------
Representation Expansion iteration #1
-----------------
Total LSTD Time = 5(s)
Running Policy Iteration:
1: 0(s), ||w1-w2|| = 0.0244, Sparsity=98.2%, -1.000 Return, 10 Steps, 42 Features
2: 0(s), ||w1-w2|| = 0.0000, Sparsity=98.2%, -1.000 Return, 7 Steps, 42 Features
Total Policy Iteration Time = 0(s)
iFDD Batch: Max Relevance = 0.008
============================
10000 >>> E[0:00:37]-R[0:00:00]: Return=-1.00, Steps=8, Features = 42
============================================================
Took 0:00:37
Results	=> ././5-results.txt
